# Stellar Cartpole

> ![Cartpole Balance](../images/cart-pole-balance.gif)

[**Download the full source code on GitHub**][1] if you want to run this simulator locally. If you want to run Cartpole remotely on the Bonsai Platform as a managed simulator, create a new BRAIN selecting the Cartpole demo on [beta.bons.ai][3].

This version of Cartpole expands on the OpenAI gym version of cartpole and exposes machine teaching logic and the rendering modeled by the classic cart-pole system implemented by Rich Sutton et al. This example will showcase the simulator file, connector file, and machine teaching file to explain how each piece is used in conjunction to train a BRAIN.

Cartpole is a classic control problem. [OpenAI Gym][2] describes it as:

_A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center._

## Inkling File

###### Schema

```inkling
schema GameState
    Float32 position,
    Float32 velocity,
    Float32 angle,
    Float32 rotation
end
```

The schema `GameState` names four records — `position`, `velocity`, `angle`, and `rotation` — and assigns a type to them. This information is input from the simulation.

```inkling
schema Action
   Int8{-1,1} command
end
```

The schema `Action` names a record — `action` —  and assigns it a constrained type. We have added constants to this schema to demonstrate how they can be optionally used.

```inkling
schema CartPoleConfig
    Int8 episode_length,
    UInt8 deque_size
end
```

 The schema `CartPoleConfig` names two records — `episode_length` and
 `deque_size` — and assigns each of them a type.   `episode_length` is a signed `Int8` because -1 is used for "run until pole drops".


###### Concept

```inkling
concept balance is classifier
    predicts (Action)
    follows input(GameState)
    feeds output
end
```

The concept is named `balance`, and it takes input from the simulator. That input is the records in the schema `GameState`. The balance concept outputs the move the AI should make in the simulator. This output is the record in the `Action` schema.

###### Simulator

```inkling
simulator the_simulator(CartPoleConfig)
   action (Action)
   state (GameState)
end
```

Simulator `the_simulator` gets information from three schemas. The first schema, `CartPoleConfig`, specifies the schema for configuration of the simulation. The second schema `Action` specifies the action that the AI will take in the simulation. The third schema `GameState` contains the state of the simulator that is sent to the lesson.

###### Curriculum

```inkling
curriculum balance_curriculum
   train balance

   with simulator the_simulator
   objective balance_objective

       lesson balancing
           configure
               constrain episode_length with Int8{-1},
               constrain deque_size with UInt8{1}
           until
               maximize balance_objective
end
```

The curriculum's name is `balance_curriculum`. It trains the `balance` concept with simulator `the_simulator`. The objective for this curriculum is `balance_objective`. The objective measures how long the pole stays upright.

This curriculum contains one lesson, called `balancing`. It configures the simulation, by setting two constraints for the state of the simulator. The lesson trains until the AI has maximized the objective.

## Simulator File

> cartpole.py

```python
import math
import numpy as np
import seeding
from bonsai_ai.logger import Logger

log = Logger()

class CartPoleModel():

    def __init__(self):
        self.gravity = 9.8
        self.masscart = 1.0
        self.masspole = 0.1
        self.total_mass = (self.masspole + self.masscart)
        self.length = 0.5  # actually half the pole's length
        self.polemass_length = (self.masspole * self.length)
        self.force_mag = 10.0
        self.tau = 0.02  # seconds between state updates

        # Angle at which to fail the episode
        self.theta_threshold_radians = 12 * 2 * math.pi / 360
        self.x_threshold = 2.4

        self._seed()
        self.viewer = None
        self.state = None

        self.steps_beyond_done = None

    def _seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def _step(self, action):
        state = self.state
        x, x_dot, theta, theta_dot = state
        force = self.force_mag if action == 1 else -self.force_mag
        costheta = math.cos(theta)
        sintheta = math.sin(theta)
        temp = (
            (force + self.polemass_length * theta_dot * theta_dot * sintheta)
            / self.total_mass)
        thetaacc = (
            (self.gravity * sintheta - costheta * temp)
            / (self.length *
               (4.0/3.0 - self.masspole * costheta * costheta /
                self.total_mass)))
        xacc = (
            temp - self.polemass_length * thetaacc * costheta / self.total_mass
        )
        x = x + self.tau * x_dot
        x_dot = x_dot + self.tau * xacc
        theta = theta + self.tau * theta_dot
        theta_dot = theta_dot + self.tau * thetaacc
        self.state = (x, x_dot, theta, theta_dot)
        done = x < -self.x_threshold \
            or x > self.x_threshold \
            or theta < -self.theta_threshold_radians \
            or theta > self.theta_threshold_radians
        done = bool(done)

        if not done:
            reward = 1.0
        elif self.steps_beyond_done is None:
            # Pole just fell!
            self.steps_beyond_done = 0
            reward = 1.0
        else:
            if self.steps_beyond_done == 0:
                log.warning("You are calling 'step()' even though this \
                environment has already returned done = True. You should \
                always call 'reset()' once you receive 'done = True' -- \
                any further steps are undefined behavior.")
            self.steps_beyond_done += 1
            reward = 0.0

        return (np.array(self.state),
                reward,
                done,
                {"steps_beyond_done": self.steps_beyond_done})

    def _reset(self):
        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
        self.steps_beyond_done = None
        return np.array(self.state)
```

This simulator file is derived from OpenAI Gym's cartpole environment, but re-factored to have minimal dependency, and rendering factored into a separate file, `render.py`. This file contains the actual simulation of the pole balancing on the cart.

## Connector File

> bridge.py

```python
import sys
import bonsai_ai
import star
from cartpole import CartPoleModel
from render import _renderer

log = bonsai_ai.logger.Logger()

star = star.Star()
model = CartPoleModel()
the_renderer = None


class ModelConnector(bonsai_ai.Simulator):
    """ A basic simulator class that takes in a move from the inkling file,
    and returns the state as a result of that move.
    """
    info = {}
    started = False
    reward = None
    terminal = None

    def __init__(self, brain, name, config):
        super(ModelConnector, self).__init__(brain, name)

    def episode_start(self, parameters=None):
        """ called at the start of every episode. should
        reset the simulation and return the initial state
        """
        log.info('Episode {} Starting'.format(self.episode_count))
        state = model._reset()
        state = star.reset(state)
        # print out a message for our first episode
        if not self.started:
            self.started = True
            print('started.')

        return state

    def simulate(self, brain_action):
        """ run a single step of the simulation.
        if the simulation has reached a terminal state, mark it as such.
        """
        action = star.action(brain_action)
        (model_state,
         model_reward,
         model_terminal,
         model_info) = model._step(action)
        self.terminal = star.terminal(model_state)
        self.info = model_info
        self.reward = star.reward(model_terminal)
        brain_state = star.state(model_state)

        if the_renderer is not None:
            the_renderer._render()

        return (brain_state, self.reward, self.terminal)

    def get_terminal(self):
        return self.terminal


if __name__ == "__main__":
    config = bonsai_ai.Config(sys.argv)
    brain = bonsai_ai.Brain(config)
    bridge = ModelConnector(brain, 'the_simulator', config)

    if '--render' in sys.argv:
        log.info('rendering')
        the_renderer = _renderer(model)

    log.info('starting simulation...')
    while bridge.run():
        continue
```

This file contains all of the connection code, or Python bridge code, to connect the model with the Bonsai Platform. This code facilitates the training of the BRAIN.

## Machine Teaching File

> star.py

```python
import math
import random
from bonsai_ai.logger import Logger

log = Logger()

class Star():
    iteration_count = 0
    model_state = None
    steps_beyond_done = None

    def __init__(self):
        # Angle at which to fail the episode
        self.theta_threshold_radians = 12 * 2 * math.pi / 360
        self.x_threshold = 2.4

    def state(self, model_state):
        """ This function converts the simulator state into the state
        representation the brain uses to learn. """
        self.model_state = model_state
        brain_state = {
            'position': model_state[0],
            'velocity': model_state[1],
            'angle': model_state[2],
            'rotation': model_state[3],
        }
        print(brain_state)
        return brain_state

    def terminal(self, model_state):
        """ Terminal conditions specify when to end an episode, typically due
        to success, failure, or running out of time. In this case, we only
        terminate when the pole falls down. """
        print(model_state)
        x, x_dot, theta, theta_dot = model_state
        done = (x < -self.x_threshold or
                x > self.x_threshold or
                theta < -self.theta_threshold_radians or
                theta > self.theta_threshold_radians)
        done = bool(done)
        return done

    def action(self, brain_action):
        """ This function converts the action representation the brain learns
        to use into the action representation the simulator uses to act in the
        simulated environment. """
        self.iteration_count += 1
        model_action = 0
        if brain_action['command'] > 0:
            model_action = 1
        return model_action

    def reward(self, done):
        """ Give the brain feedback on how well it is doing in this episode.
        In this case, this is simply 1 every time period that the pole is
        balanced. The brain's job is to learn to maximize the reward it gets
        during the episode, which corresponds to balancing as long as possible.
        """
        if not done:
            reward = 1.0
        elif self.steps_beyond_done is None:
            # Pole just fell!
            self.steps_beyond_done = 0
            reward = 1.0
        else:
            if self.steps_beyond_done == 0:
                log.info("You are calling 'step()' even though this \
                         environment has already returned done = True. \
                         You should always call 'reset()' once you receive \
                         'done = True' -- any further steps are undefined \
                         behavior.")
            self.steps_beyond_done += 1
            reward = 0.0
        return reward

    def reset(self, model_state):
        self.iteration_count = 0
        brain_state = self.state(model_state)
        self.steps_beyond_done = None

        return brain_state
```

This file contains the machine teaching logic, refered to as STAR, which stands for state, terminal, action, and reward functions. Through experimentation this is the code you will be changing to speed up training time or refine your reward function.

[1]: https://github.com/BonsaiAI/bonsai-sdk/tree/master/samples/openai-gym/gym-cartpole-sample
[2]: https://gym.openai.com/envs/CartPole-v1
[3]: https://beta.bons.ai/new
